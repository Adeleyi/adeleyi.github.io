---
layout: page
permalink: /NLL/
title: UT NLL
description: UT Austin Natural Language Learning Reading Group
nav: true
---

<!--, proposed by Professor <a href="https://www.cs.utexas.edu/~mooney/">Raymond Mooney</a>-->

<strong>Location</strong>: GDC 3.816 / <a href="https://utexas.zoom.us/j/2413159498">Zoom</a><br>
<strong>Presentation Signup Sheet</strong>: <a href="https://docs.google.com/spreadsheets/d/17y7wGwBkSCq4ZCLCYVTyCmM2m_zGGA6eGmTNPUc6UM4/edit?usp=sharing">here</a>

<h2>Spring 2024</h2>
<ul>
   <li><strong>Jan 26th, 2024</strong></li>
   Venkat will give a practice job talk.

   <li><strong>Feb 9th, 2024</strong></li>
   Anubrata will give a practice job talk.

   <li><strong>Feb 23rd, 2024: Venkat is leading the discussion</strong></li>
   We will be reading <a href="https://arxiv.org/pdf/2310.18168.pdf">Personas as a way to Model Truthfulness in Language Models</a>

   <li><strong>March 8th, 2024</strong></li>
   We will be reading <a href="https://arxiv.org/pdf/2402.12530.pdf">Parallel Structures in Pre-training Data Yield In-Context Learning</a>

   <li><strong>March 22nd, 2024: Juan Diego is leading the discussion</strong></li>
   We will be reading <a href="https://arxiv.org/pdf/2402.13956.pdf">Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment</a>

   <li><strong>April 19th, 2024: Prof. Sasha Rush is giving a talk on State Space Models</strong></li>
   See <a href="https://srush.github.io/annotated-s4/#part-1-state-space-models">this blog post</a> for an overview.
</ul>

<details>
   <summary><strong>Fall 2023</strong></summary>
   As usual, we will continue to focus on recent papers in NLP. At each meeting, we will have a volunteer leading the paper discussion. If you are interested in leading the paper, please let me know or simply put down your availability <a href="https://docs.google.com/spreadsheets/d/17y7wGwBkSCq4ZCLCYVTyCmM2m_zGGA6eGmTNPUc6UM4/edit?usp=sharing">here</a>.

   <ul>
      <li><strong>Aug 31st, 2023: Kanishka will join our discussions on his recent outstanding paper at ACL</strong></li>
      <a href="https://aclanthology.org/2023.acl-long.333/">Language model acceptability judgements are not always robust to context</a><br>
      Outstanding Paper at ACL 2023

      <li><strong>Sep 14th, 2023</strong></li>
      <a href="https://blender.cs.illinois.edu/paper/lmcollaboration2023.pdf">Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</a>

      <li><strong>Sep 28th, 2023: Philippe will lead the discussions</strong></li>
      <a href="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>

      <li><strong>Oct 12nd, 2023: Venkat will lead the discussions</strong></li>
      <a href="https://dl.acm.org/doi/pdf/10.1145/3581641.3584034">Scim: Intelligent Skimming Support for Scientific Papers</a>

      <li><strong>Oct 26th, 2023</strong></li>
      <a href="https://aclanthology.org/2023.acl-long.113.pdf">ELQA: A Corpus of Metalinguistic Questions and Answers about English</a>

      <li><strong>Nov 9th, 2023</strong>, Nathan Schneider will give a talk on ``Toward Natural Metalanguage Processing''</li>
      [Abstract] People don't just talk with natural language: sometimes, they talk about it. A wealth of knowledge about words, grammar, and meaning is communicated metalinguisticallyâ€”whether it's through dictionaries, language learning resources, scholarly works in linguistics and literature, or social/political/legal discourse. Are current NLP models fluent in metalanguage, and can they provide accurate metalinguistic explanations? I will present case studies looking at two metalinguistically rich genres: (i) online language discussion forums [ACL 2023], and (ii) judicial rulings involving language interpretation. We find that large language models can largely categorize kinds of metalanguage, and can generate satisfactory answers to some (but not all) metalinguistic questions. (Joint work with Shabnam Behzad, Michael Kranzlein, Keisuke Sakaguchi, Kevin Tobia, and Amir Zeldes.)
   </ul>
</details>

<details>
   <summary><strong>Spring 2023</strong></summary>
   This semester, we will focus on recent papers in NLP. At each meeting, one student should sign up for giving a short initial summary of the paper and for preparing some questions to get the discussion going. We will also allocate the last two meetings for students to talk about their own research. Students can give an update about their ongoing research in the form of a 10-min presentation. If you are interested in leading the paper or giving a presentation, please fill in your availability <a href="https://docs.google.com/spreadsheets/d/17y7wGwBkSCq4ZCLCYVTyCmM2m_zGGA6eGmTNPUc6UM4/edit?usp=sharing">here</a>.

   The meetings are held bi-weekly on Mondays 11:00 AM - 12:00 PM, starting from Jan 23rd. The meetings will be in hybrid, both at GDC 3.816 and via <a href="https://utexas.zoom.us/j/2413159498">Zoom</a>.

   <ul>
      <li><strong>Jan 23rd, 2023: Hongli will lead the paper discussions</strong></li>
      <a href="https://aclanthology.org/2022.emnlp-main.14/">Interpreting Language Models with Contrastive Explanations</a>, EMNLP 2022

      <li><strong>Feb 6th, 2023: Hongli will lead the paper discussions</strong></li>
      <a href="https://aclanthology.org/2022.emnlp-main.248/">Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</a>, EMNLP 2022

      <li><strong>Feb 20th, 2023: Kyle will join the paper discussions</strong></li>
      <a href="https://arxiv.org/abs/2301.06627">Dissociating language and thought in large language models: a cognitive perspective</a>

      <li><strong>Mar 6th, 2023: Venkat will lead the paper discussions</strong></li>
      <a href="https://arxiv.org/abs/2302.07459">The Capacity for Moral Self-Correction in Large Language Models</a>

      <li><strong>Mar 20th, 2023: Juan Diego will lead the paper discussions</strong></li>
      <a href="https://arxiv.org/pdf/2205.11482">Towards Tracing Factual Knowledge in Language Models Back to the Training Data</a>

      <li><strong>April 3rd, 2023: Anubrata	will lead the paper discussions</strong></li>
      <a href="https://arxiv.org/abs/2302.12389">Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support</a>

      <li><strong>April 17th, 2023: Hongli, Venkat, and Jierui will give research updates</strong></li>
   </ul>
</details>